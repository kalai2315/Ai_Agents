{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPr4fL1uRhO/0eJLJ4IZMB5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kalai2315/Ai_Agents/blob/main/Pdf_Extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ftfy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v76ngs9eMrSX",
        "outputId": "950a01b1-a058-4df8-c5ed-bf8d3b1eb8e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ftfy\n",
            "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy) (0.2.13)\n",
            "Downloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/44.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ftfy\n",
            "Successfully installed ftfy-6.3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ftfy import fix_text"
      ],
      "metadata": {
        "id": "Hzv8ZE4fMybI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-uVTHGwCQ_tg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebc93eb9-d0dd-4e3c-dc1d-a4d3b49cdf83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/47.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m47.9/47.9 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m289.1/289.1 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m10.3/10.3 MB\u001b[0m \u001b[31m87.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m57.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m129.3/129.3 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q llama-parse google-generativeai tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "from llama_parse import LlamaParse"
      ],
      "metadata": {
        "id": "YalrMtwpRCJM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "os.environ[\"LLAMA_CLOUD_API_KEY\"] = getpass(\"Enter your llama-cloud-api-key: \")"
      ],
      "metadata": {
        "id": "i_7QyFH8RNJe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cda6c373-2208-4ee0-9fd3-26a2b145996e"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your llama-cloud-api-key: ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import google.generativeai as genai\n",
        "\n",
        "# Set environment variable directly in the notebook (safe if you're not sharing)\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyBPjwNRjZm5dzdZJr_E99gkizI2qfsz05o\"\n",
        "\n",
        "# Then configure using it\n",
        "genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])"
      ],
      "metadata": {
        "id": "1_sltk_LRSdc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = genai.GenerativeModel(\"gemini-2.0-flash\")"
      ],
      "metadata": {
        "id": "HkMxvzw1RmW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "PDF_PATH = \"/content/kech103.pdf\"\n",
        "\n",
        "# Parse the PDF\n",
        "parser = LlamaParse(result_type=\"text\", verbose=True)\n",
        "documents = parser.load_data(PDF_PATH)"
      ],
      "metadata": {
        "id": "az13onY1R8J_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b18c72b-aa61-4323-95c3-f47339ad427f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Started parsing the file under job_id 4e5b1812-e20d-4b91-929c-991ea4826c60\n",
            "."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Total parsed pages: {len(documents)}\")"
      ],
      "metadata": {
        "id": "T2ClDm3ER_iy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43bf3c5f-2cd1-4bad-a7ba-8c5b935eed84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total parsed pages: 26\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i, doc in enumerate(documents[:], start=1):\n",
        "    print(f\"\\n--- Page {i} ---\\n\")\n",
        "    print(doc.text.strip())"
      ],
      "metadata": {
        "id": "U-iRAjwsSCY2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# combine every page into one big string\n",
        "#full_text = \"\\n\\n\".join(d.text for d in documents)\n",
        "#print(f\"Total characters extracted: {len(full_text):,}\")"
      ],
      "metadata": {
        "id": "7ayUNs4bSGNY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "instruction_prompt = \"\"\"\n",
        "You are a professional digital content extractor and formatter specialized in structured scientific textbooks.\n",
        "\n",
        "From the provided digitally printed **Class¬†11¬†Chemistry NCERT textbook PDF**, extract the content faithfully and exactly as printed, while preserving:\n",
        "\n",
        "Logical and Visual Structure\n",
        "- Maintain the original textbook hierarchy:\n",
        "  - chapter_number and chapter_title\n",
        "  - topic_number and topic_title\n",
        "  - subtopic_number and subtopic_title (merge into topic_number and topic_title)\n",
        "- Group all content ‚Äî including text, chemical formulas, equations, tables, images, and exercises ‚Äî under the correct topic or subtopic.\n",
        "- Maintain sequential reading order and page structure.\n",
        "- Do **NOT** skip boxed content, reaction mechanisms, headers, or diagrams.\n",
        "\n",
        "üìÑ Special Case\n",
        "- If a page has no topic title or number, extract the full visible content as‚Äëis under `content_type = \"text\"`.\n",
        "\n",
        "Content to Extract\n",
        "\n",
        "Chemical Formulas and Equations (in Text)\n",
        "- Extract **all chemical formulas, balanced equations, ionic equations, redox processes, thermochemical expressions, and reaction conditions** exactly as printed.\n",
        "- After each formula or equation, add a **plain English explanation inside brackets**, directly after it.\n",
        "  - Do **NOT** alter subscripts/superscripts (e.g., `H‚ÇÇO`, `Na‚Å∫`, `SO‚ÇÑ¬≤‚Åª`).\n",
        "  - Do **NOT** use LaTeX, arrows, or markdown.\n",
        "\n",
        "Reaction Conditions, States, and Stoichiometry\n",
        "Preserve **all symbols**, arrows, and state notations precisely. For example:\n",
        "  - `2H‚ÇÇ(g) + O‚ÇÇ(g) ‚Üí 2H‚ÇÇO(l) (hydrogen gas combines with oxygen gas to form liquid water)`\n",
        "  - `CaCO‚ÇÉ(s)  ‚Üí[Œî]  CaO(s) + CO‚ÇÇ(g) (on heating, solid calcium carbonate decomposes to calcium oxide and carbon dioxide gas)`\n",
        "  - `Fe¬≥‚Å∫ + 3e‚Åª ‚Üí Fe (iron(III) ion gains three electrons to form iron metal)`\n",
        "  - `ŒîH = ‚Äì285.8‚ÄØkJ‚ÄØmol‚Åª¬π (reaction enthalpy is minus 285.8 kilojoules per mole)`\n",
        "  - Multiple steps:\n",
        "    ```\n",
        "    Step‚ÄØ1: CH‚ÇÉ‚ÄìCH‚ÇÇ‚ÄìBr + OH‚Åª ‚Üí CH‚ÇÉ‚ÄìCH‚ÇÇ‚ÄìOH + Br‚Åª\n",
        "    Step‚ÄØ2: CH‚ÇÉ‚ÄìCH‚ÇÇ‚ÄìOH + H‚ÇÇSO‚ÇÑ ‚Üí CH‚ÇÇ=CH‚ÇÇ + H‚ÇÇO\n",
        "    ```\n",
        "    should be extracted as two separate lines with explanations in brackets.\n",
        "If any formula or mechanism appears purely as an image (e.g., curved‚Äëarrow mechanism, orbital diagram), extract a **best‚Äëeffort text approximation** using Unicode, label it as `image`, and clearly indicate interpretation.\n",
        "\n",
        "Figures (Apparatus, Molecular Structures, Graphs)\n",
        "- Extract figure number and caption.\n",
        "- Verbally describe labels, axes, molecular geometry, bonds, reaction pathways, or any annotations.\n",
        "- If equations or symbols appear in the figure, describe them **within the `image` block** (do not include in `text`).\n",
        "- Format:\n",
        "  `<image>Figure¬†3.2 ‚Äì Apparatus for preparation of hydrogen chloride gas: flask containing NaCl(s) and H‚ÇÇSO‚ÇÑ(l), delivery tube leading to gas jar inverted over mercury</image>`\n",
        "- Use:\n",
        "  - `content_type = \"image\"`\n",
        "  - `content = \"<image>...</image>\"`\n",
        "\n",
        "Tables (Data, Periodic Properties, Thermodynamic Values)\n",
        "- Extract table number and description.\n",
        "- Describe **row/column structure**, **chemical values**, and **any formulas** inside the table (e.g., ionization enthalpies, atomic radii, solubility data).\n",
        "- Examples:\n",
        "  `<table>Table¬†7.1 ‚Äì First and second ionization enthalpies (kJ‚ÄØmol‚Åª¬π) for elements Li to Ne showing general increase across the period</table>`\n",
        "  `<table>Table¬†11.2 ‚Äì Solubility product (K_sp) values of sparingly soluble salts at 298‚ÄØK</table>`\n",
        "- Use:\n",
        "  - `content_type = \"table\"`\n",
        "  - `content = \"<table>...</table>\"`\n",
        "\n",
        "Exercises\n",
        "- Include all solved and unsolved problems from within the topic, under `content_type = \"text\"`.\n",
        "\n",
        "Summary\n",
        "- Wrap full chapter summaries with:\n",
        "  `<summary>...</summary>`\n",
        "\n",
        "Output Format (JSON‚ÄëCompatible)\n",
        "\n",
        "**Text with chemical equation and explanation**:\n",
        "```json\n",
        "{\n",
        "  \"chapter_number\": \"4\",\n",
        "  \"chapter_title\": \"Chemical Bonding and Molecular Structure\",\n",
        "  \"topic_number\": \"4.3\",\n",
        "  \"topic_title\": \"Ionic Bond\",\n",
        "  \"content_type\": \"text\",\n",
        "  \"content\": \"Na(g) ‚Üí Na‚Å∫(g) + e‚Åª (g) (sodium atom loses one electron to form a sodium ion in the gaseous state)\"\n",
        "}\"\"\"\n"
      ],
      "metadata": {
        "id": "oR5HT5xnSRBt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#extract_response = model.generate_content([instruction_prompt,full_text])"
      ],
      "metadata": {
        "id": "zAca-2WhSU19"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print(extract_response.text)"
      ],
      "metadata": {
        "id": "akO28vYzSW8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PAGE_BLOCK = 5\n",
        "\n",
        "def chunk_pages(docs, block=5):\n",
        "    for i in range(0, len(docs), block):\n",
        "        yield docs[i : i + block], i + 1, i + block"
      ],
      "metadata": {
        "id": "AbHb-1uFSpyA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = []\n",
        "\n",
        "for block_idx, (pages, start_pg, end_pg) in enumerate(chunk_pages(documents, PAGE_BLOCK), start=1):\n",
        "    block_text = \"\\n\\n\".join(p.text for p in pages)\n",
        "    meta = f\"<!-- block:{block_idx} pages:{start_pg}-{end_pg} -->\\n\"\n",
        "\n",
        "    print(f\"Processing Pages {start_pg}-{end_pg}\")\n",
        "\n",
        "    response = model.generate_content([\n",
        "        meta + instruction_prompt,\n",
        "        block_text\n",
        "    ])\n",
        "\n",
        "    content = response.text.strip()\n",
        "    results.append((block_idx, start_pg, end_pg, content))"
      ],
      "metadata": {
        "id": "hsUz9e9hSW50",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "7e5cb1d5-9104-4a05-ae83-678f2d76b7e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing Pages 1-5\n",
            "Processing Pages 6-10\n",
            "Processing Pages 11-15\n",
            "Processing Pages 16-20\n",
            "Processing Pages 21-25\n",
            "Processing Pages 26-30\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"```json\")\n",
        "print(content)\n",
        "print(\"```\")"
      ],
      "metadata": {
        "id": "Qc_CSTpsSWzb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69a1f153-215f-44dd-8b76-e2630a186dbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "```json\n",
            "```json\n",
            "{\n",
            "  \"chapter_number\": \"3\",\n",
            "  \"chapter_title\": \"CLASSIFICATION OF ELEMENTS AND PERIODICITY IN PROPERTIES\",\n",
            "  \"topic_number\": \"3.35\",\n",
            "  \"topic_title\": \"Anything that influences the valence electrons will affect the chemistry of the element. Which one of the following factors does not affect the valence shell?\",\n",
            "  \"content_type\": \"text\",\n",
            "  \"content\": \"(a) Valence principal quantum number (n)\\n(b) Nuclear charge (Z )\\n(c) Nuclear mass\\n(d) Number of core electrons.\"\n",
            "}\n",
            "```\n",
            "```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for block_idx, start_pg, end_pg, content in results:\n",
        "    file_name = f\"Chemistry_block_{start_pg}_{end_pg}.txt\"\n",
        "    with open(file_name, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(\"```json\\n\")\n",
        "        f.write(content + \"\\n\")\n",
        "        f.write(\"```\\n\")\n",
        "    print(f\"Saved: {file_name}\")"
      ],
      "metadata": {
        "id": "lNxN2VloSWwo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea4bef64-dcbb-48a3-8823-c0075a2a0183"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved: Chemistry_block_1_5.txt\n",
            "Saved: Chemistry_block_6_10.txt\n",
            "Saved: Chemistry_block_11_15.txt\n",
            "Saved: Chemistry_block_16_20.txt\n",
            "Saved: Chemistry_block_21_25.txt\n",
            "Saved: Chemistry_block_26_30.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"Chemistry_unit2_output.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for block_idx, start_pg, end_pg, content in results:\n",
        "        f.write(f\"<!-- block:{block_idx} pages:{start_pg}-{end_pg} -->\\n\")\n",
        "        f.write(\"```json\\n\")\n",
        "        f.write(content + \"\\n\")\n",
        "        f.write(\"```\\n\\n\")\n",
        "print(\"All JSON chunks saved to Chemistry_unit2_output.txt\")"
      ],
      "metadata": {
        "id": "K71s-kjqS6tF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63301624-2ca8-4e5a-fd56-fabf28049fec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All JSON chunks saved to Chemistry_unit2_output.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------- Imports ----------------\n",
        "import json, re, os, html\n",
        "from pathlib import Path\n",
        "from collections import defaultdict\n",
        "import pandas as pd\n",
        "from ftfy import fix_text   # pip install ftfy\n",
        "\n",
        "# ---------------- File Paths ----------------\n",
        "INPUT_TXT   = \"/content/Chemistry_block_26_30.txt\"\n",
        "OUTPUT_CSV  = \"Chemistry_csv/u3_chunk26-30.csv\"\n",
        "BROKEN_LOG  = \"logs/u3_broken_chunk26-30.jsonl\"\n",
        "\n",
        "# ---------------- Housekeeping ----------------\n",
        "os.makedirs(Path(OUTPUT_CSV).parent, exist_ok=True)\n",
        "os.makedirs(Path(BROKEN_LOG).parent, exist_ok=True)\n",
        "\n",
        "# ---------------- 1. Load Input Text ----------------\n",
        "raw_text = Path(INPUT_TXT).read_text(encoding=\"utf-8\")\n",
        "\n",
        "# ---------------- 2. Normalize ```json fences ----------------\n",
        "raw_text = re.sub(r\"```json\\s*```json\", \"```json\", raw_text)\n",
        "raw_text = re.sub(r\"```json\\s*{\", \"```json\\n[ {\", raw_text)\n",
        "raw_text = re.sub(r\"}\\s*```\", \"} ]\\n```\", raw_text)\n",
        "\n",
        "# ---------------- 3. Extract JSON Blocks ----------------\n",
        "json_blocks = re.findall(r\"```json\\s*(\\[\\s*{.*?}\\s*])\\s*```\", raw_text, flags=re.DOTALL)\n",
        "\n",
        "all_records = []\n",
        "broken_blocks = []\n",
        "\n",
        "# ---------------- 4. Parse and Clean Each Block ----------------\n",
        "for i, block in enumerate(json_blocks, start=1):\n",
        "    try:\n",
        "        cleaned = block.strip().replace('\\x00', '').replace('\\x1a', '')\n",
        "        cleaned = html.unescape(cleaned)\n",
        "        cleaned = re.sub(r'[\\x00-\\x1F\\x7F]', '', cleaned)\n",
        "\n",
        "        records = json.loads(cleaned)\n",
        "\n",
        "        for rec in records:\n",
        "            if isinstance(rec.get(\"content\"), str):\n",
        "                rec[\"content\"] = fix_text(rec[\"content\"])\n",
        "            all_records.append(rec)\n",
        "\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"JSON Decode Error in block {i}: {e}\")\n",
        "        broken_blocks.append({\n",
        "            \"block_index\": i,\n",
        "            \"error\": str(e),\n",
        "            \"snippet\": block[:300]\n",
        "        })\n",
        "\n",
        "# ---------------- 5. Group by Structure ----------------\n",
        "grouped = defaultdict(list)\n",
        "for record in all_records:\n",
        "    key = (\n",
        "        (record.get(\"chapter_number\") or \"\").strip(),\n",
        "        (record.get(\"chapter_title\") or \"\").strip(),\n",
        "        (record.get(\"topic_number\") or \"\").strip(),\n",
        "        (record.get(\"topic_title\") or \"\").strip(),\n",
        "        (record.get(\"content_type\") or \"\").strip()\n",
        "    )\n",
        "    grouped[key].append((record.get(\"content\") or \"\").strip())\n",
        "\n",
        "# ---------------- 6. Flatten for CSV ----------------\n",
        "flattened_records = []\n",
        "for (chap_num, chap_title, topic_num, topic_title, content_type), contents in grouped.items():\n",
        "    full_content = \"\\n\\n\".join(contents)\n",
        "    flattened_records.append({\n",
        "        \"subject\": \"Chemistry\",\n",
        "        \"chapter_number\": chap_num,\n",
        "        \"chapter_title\": chap_title,\n",
        "        \"topic_number\": topic_num,\n",
        "        \"topic_title\": topic_title,\n",
        "        \"content_type\": content_type,\n",
        "        \"content\": full_content\n",
        "    })\n",
        "\n",
        "# ---------------- 7. Save to CSV ----------------\n",
        "if flattened_records:\n",
        "    pd.DataFrame(flattened_records).to_csv(OUTPUT_CSV, index=False, quoting=1)  # QUOTE_ALL\n",
        "    print(f\"Final grouped CSV written to: {OUTPUT_CSV} ({len(flattened_records)} rows)\")\n",
        "else:\n",
        "    print(\"No valid records found in JSON.\")\n",
        "\n",
        "# ---------------- 8. Log Broken Chunks ----------------\n",
        "if broken_blocks:\n",
        "    with open(BROKEN_LOG, \"w\", encoding=\"utf-8\") as f:\n",
        "        for item in broken_blocks:\n",
        "            f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
        "    print(f\"Logged {len(broken_blocks)} broken JSON chunks ‚Üí {BROKEN_LOG}\")\n",
        "else:\n",
        "    print(\"No JSON errors in any block.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8g7jI7y8NUUI",
        "outputId": "8a6c38a2-d7c7-4de3-d800-80c5e2dc34fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final grouped CSV written to: Chemistry_csv/u3_chunk26-30.csv (1 rows)\n",
            "No JSON errors in any block.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Folder path\n",
        "folder_path = \"/content/Chemistry_csv\"\n",
        "\n",
        "# Correct order of files\n",
        "ordered_files = [\n",
        "    \"u3_chunk1-5.csv\",\n",
        "    \"u3_chunk6-10.csv\",\n",
        "    \"u3_chunk11-15.csv\",\n",
        "    \"u3_chunk16-20.csv\",\n",
        "    \"u3_chunk21-25.csv\",\n",
        "    \"u3_chunk26-30.csv\"\n",
        "\n",
        "]\n",
        "\n",
        "# Read and concatenate\n",
        "dfs = [pd.read_csv(os.path.join(folder_path, file)) for file in ordered_files]\n",
        "final_df = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "# Optional: Save to a single file\n",
        "final_df.to_csv(\"Chemistry_Unit3_Combined.csv\", index=False)\n",
        "\n",
        "print(\"‚úÖ All CSVs appended in order and saved to 'Chemistry_Unit3_Combined.csv'\")\n"
      ],
      "metadata": {
        "id": "y1zmeQFKNmQE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b0b5014-2804-4f32-9845-287b19a52a82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ All CSVs appended in order and saved to 'Chemistry_Unit3_Combined.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load your CSV file\n",
        "df = pd.read_csv(\"/content/Chemistry_Unit3_Combined.csv\")\n",
        "\n",
        "# Replace unwanted characters\n",
        "df = df.replace({'‚Äö√Ñ¬¢': '‚Ä¢', '‚Äö√Ñ√Æ': '‚Äî'}, regex=True)\n",
        "\n",
        "# Save the cleaned file\n",
        "df.to_csv(\"Chemistry_Unit3_FinalCleaned.csv\", index=False)"
      ],
      "metadata": {
        "id": "W54_BTXKNoeO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import google.generativeai as genai\n",
        "import time\n",
        "\n",
        "# Configure your Gemini API key\n",
        "genai.configure(api_key=\"***********\")\n",
        "\n",
        "# Load the input CSV\n",
        "df = pd.read_csv(\"Chemistry_Unit3_Combined.csv\")  # Replace with your actual CSV file\n",
        "\n",
        "# Prepare Gemini model\n",
        "model = genai.GenerativeModel(\"gemini-2.0-flash\")\n",
        "\n",
        "# Define prompt template\n",
        "def build_prompt(text):\n",
        "    return f\"\"\"\n",
        "You are a curriculum designer.\n",
        "\n",
        "Generate Bloom's taxonomy-based learning objectives from the following textbook content.\n",
        "\n",
        "Instructions:\n",
        "- Only include categories from Bloom‚Äôs Taxonomy (Remember, Understand, Apply, Analyze, Evaluate, Create) that are relevant based on the depth and nature of the content provided.\n",
        "- Do NOT generate objectives for categories that do not align with the content‚Äôs complexity or conceptual level.\n",
        "- Focus on important, conceptually meaningful ideas ‚Äî avoid trivial or superficial facts (e.g., dates, names, unless critical to understanding).\n",
        "- Use strong action verbs appropriate to each Bloom‚Äôs level.\n",
        "- Do not invent content beyond what is given ‚Äî derive objectives faithfully from the text.\n",
        "- Wrap objectives using clean HTML tags, with this format:\n",
        "  <Understand>\n",
        "  1. ...\n",
        "  2. ...\n",
        "  </Understand>\n",
        "  <Apply>\n",
        "  1. ...\n",
        "  </Apply>\n",
        "  (Use only relevant categories; do not include all six by default.)\n",
        "- The number of objectives per category should be dynamic based on the depth and importance of the content.\n",
        "- Ensure all objectives are clear, concise, and free from extra formatting (no **bold**, no markdown, no bullet symbols).\n",
        "- Do not include explanations, summaries, or preambles ‚Äî return only the learning objectives in the specified HTML-tagged format.\n",
        "\n",
        "Content:\n",
        "\\\"\\\"\\\"{text}\\\"\\\"\\\"\n",
        "\"\"\"\n",
        "\n",
        "# Generate learning objectives with retry & rate-limit handling\n",
        "def generate_learning_objectives(text):\n",
        "    prompt = build_prompt(text)\n",
        "    try:\n",
        "        response = model.generate_content(prompt)\n",
        "        return response.text.strip()\n",
        "    except Exception as e:\n",
        "        return f\"Error: {str(e)}\"\n",
        "\n",
        "# Apply to each row with delay to avoid rate limit\n",
        "results = []\n",
        "for i, row in df.iterrows():\n",
        "    print(f\"Processing row {i+1} of {len(df)}...\")\n",
        "    objectives = generate_learning_objectives(str(row['content']))\n",
        "    results.append(objectives)\n",
        "    time.sleep(5)  # wait to avoid 429 error\n",
        "\n",
        "# Add to DataFrame\n",
        "df['learning_objectives'] = results\n",
        "\n",
        "# Save output\n",
        "df.to_csv(\"Chemistry_Unit3_Combined.csv\", index=False)\n",
        "print(\"Output saved to 'CHEMISTRY1.csv'\")\n"
      ],
      "metadata": {
        "id": "kcIaMlhyfBLH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}